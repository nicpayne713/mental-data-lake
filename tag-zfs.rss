<?xml version="1.0" encoding="utf-8"?><rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Pype.dev</title><link>https://pype.dev</link><description>my mental data-lake</description><pubDate>Wed, 11 Dec 2024 10:48:10 GMT</pubDate><lastBuildDate>Sat, 25 Jan 2025 19:52:49 GMT</lastBuildDate><generator>marmite</generator><image><url>https://pype.dev/media/og-02.png</url><title></title><link></link></image><item><title>D and uninterruptable sleep</title><link>https://pype.dev/d-and-uninterruptable-sleep.html</link><author>nicpayne</author><category>linux</category><category>zfs</category><category>tech</category><guid>https://pype.dev/d-and-uninterruptable-sleep.html</guid><pubDate>Wed, 11 Dec 2024 10:48:10 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h2><a href="#htop" aria-hidden="true" class="anchor" id="htop"></a>Htop</h2>
<p>I recently have been having significant home server issues, and that's not the point of this - today I learned what <code>D</code> state is when looking at htop.</p>
<img src="https://cdn.statically.io/gh/pypeaday/images.pype.dev/main/blog-media/d-htop.png" alt="htop-d" title="htop with D state" />
<p>Apparently this means &quot;uninterruptable sleep&quot; and it's a dev's nightmare...</p>
<h3><a href="#context" aria-hidden="true" class="anchor" id="context"></a>Context</h3>
<p>The issue I was having was that some <code>zfs rollback</code> commands were hung - for hours... I wasn't sure what was going on, rollbacks should be instant but I figured it was just an artifact of these issues.</p>
<p>Turns out I still don't know what locked the disks up but I learned why <code>&lt;C&gt;-c</code> did <u>nothing</u>...
the more you know</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>lsof to find what&apos;s using your filesystem</title><link>https://pype.dev/lsof-to-find-what-s-using-your-filesystem.html</link><author>nicpayne</author><category>zfs</category><category>homelab</category><category>tech</category><guid>https://pype.dev/lsof-to-find-what-s-using-your-filesystem.html</guid><pubDate>Sun, 09 Apr 2023 13:32:38 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<p>lsof | grep /tank/nas shows me what is using my nas at any time!</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Changing ZFS key for child datasets of encrypted dataset after migration</title><link>https://pype.dev/changing-zfs-key-for-child-datasets-of-encrypted-dataset-after-migration.html</link><author>nicpayne</author><category>zfs</category><category>homelab</category><category>tech</category><guid>https://pype.dev/changing-zfs-key-for-child-datasets-of-encrypted-dataset-after-migration.html</guid><pubDate>Sat, 08 Apr 2023 20:12:22 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<p>➜ pihole sudo zfs load-key -L file:///path/to/.zfs.tank.key tank/encrypted/vms/arch-sandbox
➜ pihole sudo zfs change-key -o keylocation=file:///path/to/.zfs.tank.key -o keyformat=raw tank/encrypted/vms/arch-sandbox
Need to load-key for each individual dataset, then change key location to be a file instead of the prompt</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Systemd timer for syncoid</title><link>https://pype.dev/systemd-timer-for-syncoid.html</link><author>nicpayne</author><category>zfs</category><category>homelab</category><category>tech</category><guid>https://pype.dev/systemd-timer-for-syncoid.html</guid><pubDate>Wed, 21 Dec 2022 11:38:27 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<p>I have a bash script called <code>syncoid-job</code> which boils down to a barebones -</p>
<pre><code class="language-bash">#!/bin/bash

syncoid --no-sync-snap --sendoptions=w --no-privilege-elevation $SYNOIC_USER@$SERVER:tank/encrypted/nas tank/encrypted/nas
</code></pre>
<p>I want to run this script hourly but as my user (notice the no-privilege-elevation flag)</p>
<p>First - create a systemd unit file at <code>/etc/systemd/system/syncoid-replication.service</code></p>
<pre><code class="language-bash">[Unit]
Description=ZFS Replication With Syncoid

[Service]
Type=oneshot
ExecStart=/$HOME/dotfiles/syncoid-job
User=$USER
Group=$GROUP

[Install]
WantedBy=multi-user.target

</code></pre>
<p>Then we save the unit file, enable the service, and then start it</p>
<pre><code class="language-console">systemctl enable syncoid-replication.service
systemctl start syncoid-replication.service

</code></pre>
<blockquote>
<p>Note this will run that script... so be ready for syncoid to do its thing</p>
</blockquote>
<p>Now for the timer... We create <code>/etc/systemd/system/syncoid-replication.timer</code></p>
<pre><code class="language-bash">[Unit]
Description=Run syncoid-replication every hour

[Timer]
OnCalendar=hourly

[Install]
WantedBy=timers.target

</code></pre>
<p>Hit it with a <code>systemctl enable syncoid-replication.timer</code> and you're in business!</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Limit zfs list to avoid docker vomit</title><link>https://pype.dev/limit-zfs-list-to-avoid-docker-vomit.html</link><author>nicpayne</author><category>zfs</category><category>cli</category><category>tech</category><guid>https://pype.dev/limit-zfs-list-to-avoid-docker-vomit.html</guid><pubDate>Thu, 20 Oct 2022 06:39:18 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<p>zfs list has a flag -r, but if you use zfs driver for docker then you'll get
flooded with every docker volume in the world. zfs list -r -d N will limit the
dept of the print out, so zfs list -r -d 2 gives me tank, tank/encrypted,
tank/encrypted/docker -&gt; but then I don't see all the continer volumes</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Quick setup of ZFS encrytped datasets with sane permissions</title><link>https://pype.dev/quick-setup-of-zfs-encrytped-datasets-with-sane-permissions.html</link><author>nicpayne</author><category>zfs</category><category>homelab</category><category>tech</category><guid>https://pype.dev/quick-setup-of-zfs-encrytped-datasets-with-sane-permissions.html</guid><pubDate>Thu, 06 Oct 2022 19:58:01 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<p>Assuming you have a pool called <code>tank</code>...</p>
<p>And assuming you have an encrypted dataset (See <a href="https://arstechnica.com/gadgets/2021/06/a-quick-start-guide-to-openzfs-native-encryption/">Jim Saltar's short
intro</a>)</p>
<ol>
<li>Create a group for permissions - in my case I have one called <code>home</code></li>
<li>Then if there's anything in <code>/tank/encrypted</code> his it with <code>chgrp -R home /tank/encrypted</code> to give the <code>home</code> group ownership</li>
<li>Next we need to make sure that the members of <code>home</code> can do the writing...
so <code>chmod 775 -R /tank/encrypted</code> will do the trick</li>
<li>Finally we want to make sure that all data created inside our dataset has
the same set of permissions with <code>chmod g+s /tank/encrypted</code> and <code>chmod g+w /tank/encrypted</code></li>
</ol>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Benchmark your disks with fio</title><link>https://pype.dev/benchmark-your-disks-with-fio.html</link><author>nicpayne</author><category>python</category><category>zfs</category><category>tech</category><guid>https://pype.dev/benchmark-your-disks-with-fio.html</guid><pubDate>Sat, 27 Aug 2022 13:43:22 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h1><a href="#intro" aria-hidden="true" class="anchor" id="intro"></a>Intro</h1>
<p>I use ZFS at home in my homelab for basically all of my storage... Docker uses
ZFS backend, all my VMs have their <code>.qcow2</code> images in their own zfs datasets,
and all my shares are ZFS datasets. I love ZFS but my home hardware presently
is the opposite of expensive or new... Thankfully I've had a lot of my orginal
homelab simply given to me but the cost of this is that I didn't put my
machines together, I didn't choose the disks, and I definitely didn't do the
research I would've otherwise done had I bankrolled my server personally...</p>
<h2><a href="#the-problem" aria-hidden="true" class="anchor" id="the-problem"></a>The Problem</h2>
<p>I run <code>glances</code> on basically all my machines and for the longest time I have
been seeing big time <code>iowait</code> issues. Now, since everything was free I've
largely been able to ignore that however I'm now after some better performance
which I think means new hardware!</p>
<p>Here is a random screenshot of my glances homepage at time of writing - The
only major load on my server is some <code>ffmpeg</code> transcoding (about 60% CPU
utilization)...</p>
<img src="https://cdn.statically.io/gh/pypeaday/images.pype.dev/main/blog-media/glances-iowait.png" alt="glances" title="glances with iowait" />
<p>As you can see... there's a lot of issues and <em>I don't even know what they mean</em>.</p>
<h1><a href="#fio" aria-hidden="true" class="anchor" id="fio"></a>fio</h1>
<p>I heard about <a href="https://fio.readthedocs.io/en/latest/">fio</a> through a friend and
decided to try it out quick. It installs with <code>apt</code> on ubuntu quick and easy...</p>
<p>Jim Saltar has a good blog post on it <a href="https://arstechnica.com/gadgets/2020/02/how-fast-are-your-disks-find-out-the-open-source-way-with-fio/">here</a></p>
<p>Basically it's a handy tool for benchmarking your disks and the blog dives into
what types of metrics matter - it's not just throughput, but also latency,
iops, etc.</p>
<h2><a href="#tests" aria-hidden="true" class="anchor" id="tests"></a>Tests</h2>
<p>I ran a few basic commands inside a new zfs dataset on my server <code>tank/fio</code></p>
<pre><code class="language-bash">fio --name=random-write --ioengine=posixaio --rw=randwrite --bs=4k --size=4g --numjobs=1 --runtime=60 --time_based --end_fsync=1 &gt; single-4KiB-random-write.txt
fio --name=random-write --ioengine=posixaio --rw=randwrite --bs=64k --size=256m --numjobs=16 --iodepth=16 --runtime=60 --time_based --end_fsync=1 &gt; 16-parallel-64KiB-random-write.txt
fio --name=random-write --ioengine=posixaio --rw=randwrite --bs=1m --size=16g --numjobs=1 --iodepth=1 --runtime=60 --time_based --end_fsync=1 &gt; single-1MiB-random-write.txt
</code></pre>
<h2><a href="#results" aria-hidden="true" class="anchor" id="results"></a>Results</h2>
<p>The single 4 KiB random write:</p>
<p><code>WRITE: bw=7836KiB/s (8024kB/s), 7836KiB/s-7836KiB/s (8024kB/s-8024kB/s), io=523MiB (548MB), run=68317-68317msec</code></p>
<p>The 16 parallel 64KiB random writes:</p>
<p><code>WRITE: bw=93.9MiB/s (98.4MB/s), 5599KiB/s-6303KiB/s (5734kB/s-6454kB/s), io=7642MiB (8013MB), run=81310-81418msec</code></p>
<p>The single 1MiB random write:</p>
<p><code>WRITE: bw=81.2MiB/s (85.1MB/s), 81.2MiB/s-81.2MiB/s (85.1MB/s-85.1MB/s), io=8177MiB (8574MB), run=100699-100699msec</code></p>
<h1><a href="#summary" aria-hidden="true" class="anchor" id="summary"></a>Summary</h1>
<p>So I don't fully understand these numbers yet... 80-100 MiB/s isn't super fast
and that's across a parallelized workload... The single threaded workloads have
awful performance so this tells me something is wrong... I have a few ideas...</p>
<ol>
<li>ZFS dataset config options such as <code>ashift</code> or the blocksize might be way misconfigured</li>
<li>The disks/pool which came from a TrueNAS/FreeBSD machine may have some artifacts that I need to clean up</li>
<li>The SAS controller I am using, which I flashed with IT firmware to get it into JBOD mode might be messed up</li>
<li>The data cables themselves could be a problem...</li>
</ol>
<p>Points 3 and 4 are less likely given that the write speed does increase in the parallelized job but I'm a newbie so it's time to dive in!</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>ZFS Permissions for Sanoid/Syncoid</title><link>https://pype.dev/zfs-permissions-for-sanoid-syncoid.html</link><author>nicpayne</author><category>zfs</category><category>homelab</category><category>tech</category><guid>https://pype.dev/zfs-permissions-for-sanoid-syncoid.html</guid><pubDate>Fri, 08 Jul 2022 15:54:58 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<p><code>zfs allow -u $USER clone,load-key,create,destroy,mount,mountpoint,receive,send,rollback,compression,snapshot,hold,keylocation,bookmark tank</code></p>
<blockquote>
<p>load-key only needed if using encrypted datasets</p>
</blockquote>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Self-hosted Docker registry with proxy pull through</title><link>https://pype.dev/self-hosted-docker-registry-with-proxy-pull-through.html</link><author>nicpayne</author><category>homelab</category><category>zfs</category><category>tech</category><guid>https://pype.dev/self-hosted-docker-registry-with-proxy-pull-through.html</guid><pubDate>Wed, 06 Jul 2022 06:44:35 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<p>I decided that I want to self-host all my docker images for the purposes of
regularly rebuilding and security scanning. The first step is to set up a
registry, which coincidently enough you can do with a Docker container 😛!</p>
<p>Instructions for setting up the proxy are on the offidical docker docs <a href="https://docs.docker.com/registry/recipes/mirror/">here</a></p>
<h1><a href="#deployment" aria-hidden="true" class="anchor" id="deployment"></a>Deployment</h1>
<p>I chose to deploy with Portainer because I already use it for monitoring all my
docker containers. I deploy most container with Ansible but use Portainer to
setup a few more adhoc or non-git driven applications.</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>How I use Nextcloud for safe central storage</title><link>https://pype.dev/how-i-use-nextcloud-for-safe-central-storage.html</link><author>nicpayne</author><category>homelab</category><category>zfs</category><category>tech</category><guid>https://pype.dev/how-i-use-nextcloud-for-safe-central-storage.html</guid><pubDate>Thu, 19 May 2022 21:17:42 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<ol>
<li>Setup admin</li>
<li>External Storage extension</li>
<li>Add my nas zfs dataset</li>
<li>chown -R www-data:www-data on anything nextcloud uploads to.</li>
</ol>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Nextcloud permissions with ZFS and Ansible-NAS</title><link>https://pype.dev/nextcloud-permissions-with-zfs-and-ansible-nas.html</link><author>nicpayne</author><category>homelab</category><category>zfs</category><category>tech</category><guid>https://pype.dev/nextcloud-permissions-with-zfs-and-ansible-nas.html</guid><pubDate>Thu, 19 May 2022 13:55:20 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<h1><a href="#tldr" aria-hidden="true" class="anchor" id="tldr"></a>TL;DR</h1>
<p>As the nextcloud docs say... if you want to write to an external volume that
location has to be writeable by the user/group <code>www-data</code> on the host system...
so if that makes sense to you then this TIL probably isn't a ton of value.. if
not however, read on :)</p>
<h1><a href="#case-study" aria-hidden="true" class="anchor" id="case-study"></a>Case Study</h1>
<p>You want to self-host your own cloud and use a smart file system for convenience...
Nextcloud and ZFS are pretty common goto answers for each of those problems.</p>
<p>My home NAS is built on ZFS and among other things I have a <code>zpool</code> named
<code>tank</code> and nested in there is a <code>tank/nas</code> dataset with several child zfs
datasets under that.</p>
<p>I want to use nextcloud mainly for auto-uploading photos from my wife's and my phones for automatic backups.
The issue is that the nextcloud application (I run in Docker) is fixed as the
<code>www-data</code> user and so any volume/folder that you want nextcloud to write to
needs to be permissioned such that <code>www-data</code> owns it... but I don't want
<code>www-data</code> to own everything in my NAS... so what's a girl to do?</p>
<h1><a href="#solution" aria-hidden="true" class="anchor" id="solution"></a>Solution</h1>
<p>Well, one way to go is to just utilize docker volumes, write the data in the
container to <code>/var/www/html</code> and let that be the place your data backsup to.</p>
<p>I still wanted nextcloud to automatically write right to my NAS so I created a
<code>nextcloud-upload</code> directory inside of <code>tank/nas/media/photos</code> (photos cause
that's all that gets automatically uploaded)</p>
<p>Then I <code>chown -R www-data:www-data /tank/nas/media/photos/nextcloud-upload</code> so
that just that sub-folder is owned by <code>www-data</code>. Now everyone's happy!</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>See ZFS snapshot disk usage</title><link>https://pype.dev/see-zfs-snapshot-disk-usage.html</link><author>nicpayne</author><category>zfs</category><category>homelab</category><category>tech</category><guid>https://pype.dev/see-zfs-snapshot-disk-usage.html</guid><pubDate>Thu, 19 May 2022 06:05:23 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<p>As I was cleaning up my NAS recently I noticed that I ran out of storage even
though my disk usage looked pretty low... turns out I was keeping a mega-ton of
ZFS snapshots and due to my own ignorance at the time didn't realize the
storage cost of this!</p>
<p><code>zfs list-o space tank/home</code> will show the disk usage of the dataset and snapshots in <code>tank/home</code></p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item><item><title>Remove ZFS Dataset Specific Snapshots</title><link>https://pype.dev/remove-zfs-dataset-specific-snapshots.html</link><author>nicpayne</author><category>linux</category><category>zfs</category><category>cli</category><category>bash</category><category>homelab</category><category>tech</category><guid>https://pype.dev/remove-zfs-dataset-specific-snapshots.html</guid><pubDate>Thu, 19 May 2022 05:49:16 GMT</pubDate><source url="https://pype.dev">tag-zfs</source><content:encoded><![CDATA[<!-- Content Injected to every content markdown header -->
<p>I started my homelab journey being super naive about ZFS and how to manage the
filesystem... that bit me in the butt when transfering a ton of files out of
folders and into datasets because ZFS is copy on write so I was essentially
duplicating my storage until I got a hair smarter about removing files after
they're moved (rsync --remove-source-file ftw). But I had a ton of snapshots of
child datasets with a ton of data that I just never will need, so I learned
<code>zfs list -H -o name -t snapshot tank/dataset1/dataset2</code> will list just the
snapshots for dataset2 and if you pipe that into <code>xargs -n1 zfs destroy</code> then
you have a way to clear out some snapshots you don't need!</p>
<!-- Content Injected to every content markdown footer -->
]]></content:encoded></item></channel></rss>